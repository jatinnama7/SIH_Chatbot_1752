{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db15c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Check if a pad token exists, if not, add it\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Resize the model's token embeddings to accommodate the new pad token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Custom dataset class to handle training data\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, tokenizer, intents, max_length=128):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for intent in intents['intents']:\n",
    "            for pattern in intent['patterns']:\n",
    "                # Add variations by slightly modifying the pattern\n",
    "                variations = self.create_variations(pattern)\n",
    "                for variation in variations:\n",
    "                    response = random.choice(intent['responses'])\n",
    "                    # Provide clear format for training with direct Q&A format\n",
    "                    input_text = f\"User: {variation}\\nBot: {response}\" \n",
    "\n",
    "                    encodings = tokenizer(\n",
    "                        input_text,\n",
    "                        truncation=True,\n",
    "                        padding=\"max_length\",\n",
    "                        max_length=max_length,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    self.input_ids.append(encodings['input_ids'][0])\n",
    "                    self.attn_masks.append(encodings['attention_mask'][0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attn_masks[idx],\n",
    "            'labels': self.input_ids[idx]  # Labels should match the input for language modeling\n",
    "        }\n",
    "\n",
    "    def create_variations(self, text):\n",
    "        # Function to create small variations of input text\n",
    "        variations = [text]\n",
    "        # Simple augmentation - Adding exclamations, question marks, etc.\n",
    "        if not text.endswith(\"?\"):\n",
    "            variations.append(text + \"?\")\n",
    "        if not text.endswith(\"!\"):\n",
    "            variations.append(text + \"!\")\n",
    "        # Add more variations if needed (like swapping synonyms, rephrasing)\n",
    "        return variations\n",
    "\n",
    "# Load intents data\n",
    "with open('intents.json') as file:\n",
    "    intents = json.load(file)\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = ChatDataset(tokenizer, intents)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Disable masked language modeling since it's not relevant for GPT-2\n",
    ")\n",
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,  # Increased epochs to allow more learning\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"no\",\n",
    "    prediction_loss_only=True\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-gpt2\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-gpt2\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
